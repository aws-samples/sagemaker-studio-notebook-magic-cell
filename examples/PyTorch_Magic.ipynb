{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name workshop-sagemaker to get Role path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "output_path='s3://' + sess.default_bucket() + '/pytorch/mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# Download training and testing data from a public S3 bucket\n",
    "\n",
    "\n",
    "def download_from_s3(data_dir='/tmp/data', train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get global config\n",
    "#     with open('code/config.json', 'r') as f:\n",
    "#         CONFIG=json.load(f)\n",
    "\n",
    "    CONFIG = {}\n",
    "    CONFIG['public_bucket'] = \"sagemaker-sample-files\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    # download objects\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = CONFIG['public_bucket']\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3('/tmp/data', True)\n",
    "download_from_s3('/tmp/data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'mnist'\n",
    "bucket = sess.default_bucket()\n",
    "loc = sess.upload_data(path='/tmp/data', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "channels = {\n",
    "    \"training\": loc,\n",
    "    \"testing\": loc\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::245582572290:role/workshop-sagemaker\n",
      "s3://sagemaker-eu-west-1-245582572290/pytorch/mnist\n",
      "s3://sagemaker-eu-west-1-245582572290/mnist\n",
      "s3://sagemaker-eu-west-1-245582572290/mnist\n"
     ]
    }
   ],
   "source": [
    "print(role)\n",
    "print(output_path)\n",
    "print(channels.get('training'))\n",
    "print(channels.get('testing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %pytorch [--estimator_name ESTIMATOR_NAME] [--entry_point ENTRY_POINT]\n",
       "               [--source_dir SOURCE_DIR] [--role ROLE]\n",
       "               [--framework_version FRAMEWORK_VERSION]\n",
       "               [--py_version PY_VERSION] [--instance_type INSTANCE_TYPE]\n",
       "               [--instance_count INSTANCE_COUNT] [--output_path OUTPUT_PATH]\n",
       "               [--hyperparameters FOO:1,BAR:0.555,BAZ:ABC | 'FOO : 1, BAR : 0.555, BAZ : ABC']\n",
       "               [--channel_training CHANNEL_TRAINING]\n",
       "               [--channel_testing CHANNEL_TESTING]\n",
       "               [--use_spot_instances [USE_SPOT_INSTANCES]]\n",
       "               [--max_wait MAX_WAIT]\n",
       "               [--enable_sagemaker_metrics [ENABLE_SAGEMAKER_METRICS]]\n",
       "               [--metric_definitions ['Name: loss, Regex: Loss = .*?);' ['Name: loss, Regex: Loss = (.*?;' ...]]]\n",
       "               [--name_contains NAME_CONTAINS] [--max_result MAX_RESULT]\n",
       "               {submit,list,status,logs,delete}\n",
       "\n",
       "Pytorch magic command.\n",
       "\n",
       "methods:\n",
       "  {submit,list,status,logs,delete}\n",
       "\n",
       "submit:\n",
       "  --estimator_name ESTIMATOR_NAME\n",
       "                        estimator shell variable name\n",
       "  --entry_point ENTRY_POINT\n",
       "                        notebook local code file\n",
       "  --source_dir SOURCE_DIR\n",
       "                        notebook local code src, may contain requirements.txt\n",
       "  --role ROLE           An AWS IAM role (either name or full ARN). The Amazon\n",
       "                        SageMaker training jobs and APIs that create Amazon\n",
       "                        SageMaker endpoints use this role to access training\n",
       "                        data and model artifacts. After the endpoint is\n",
       "                        created, the inference code might use the IAM role, if\n",
       "                        it needs to access an AWS resource.\n",
       "  --framework_version FRAMEWORK_VERSION\n",
       "                        PyTorch version\n",
       "  --py_version PY_VERSION\n",
       "                        Python version\n",
       "  --instance_type INSTANCE_TYPE\n",
       "                        Type of EC2 instance to use for training, for example,\n",
       "                        ‘ml.c4.xlarge’.\n",
       "  --instance_count INSTANCE_COUNT\n",
       "                        Number of Amazon EC2 instances to use for training.\n",
       "  --output_path OUTPUT_PATH\n",
       "                        S3 location for saving the training result (model\n",
       "                        artifacts and output files). If not specified, results\n",
       "                        are stored to a default bucket. If the bucket with the\n",
       "                        specific name does not exist, the estimator creates\n",
       "                        the bucket during the fit() method execution.\n",
       "  --hyperparameters <FOO:1,BAR:0.555,BAZ:ABC | 'FOO : 1, BAR : 0.555, BAZ : ABC'>\n",
       "                        Hyperparameters are passed to your script as arguments\n",
       "                        and can be retrieved with an argparse.\n",
       "  --channel_training CHANNEL_TRAINING\n",
       "                        A string that represents the path to the directory\n",
       "                        that contains the input data for the training channel.\n",
       "  --channel_testing CHANNEL_TESTING\n",
       "                        A string that represents the path to the directory\n",
       "                        that contains the input data for the testing channel.\n",
       "\n",
       "submit-spot:\n",
       "  --use_spot_instances <[USE_SPOT_INSTANCES]>\n",
       "                        Specifies whether to use SageMaker Managed Spot\n",
       "                        instances for training. If enabled then the max_wait\n",
       "                        arg should also be set. More information:\n",
       "                        https://docs.aws.amazon.com/sagemaker/latest/dg/model-\n",
       "                        managed-spot-training.html\n",
       "  --max_wait MAX_WAIT   Timeout in seconds waiting for spot training instances\n",
       "                        (default: None). After this amount of time Amazon\n",
       "                        SageMaker will stop waiting for Spot instances to\n",
       "                        become available (default: None).\n",
       "\n",
       "submit-metrics:\n",
       "  --enable_sagemaker_metrics <[ENABLE_SAGEMAKER_METRICS]>\n",
       "                        Enables SageMaker Metrics Time Series. For more\n",
       "                        information see: https://docs.aws.amazon.com/sagemaker\n",
       "                        /latest/dg/API_AlgorithmSpecification.html# SageMaker-\n",
       "                        Type-AlgorithmSpecification-\n",
       "                        EnableSageMakerMetricsTimeSeries\n",
       "  --metric_definitions <['Name: loss, Regex: Loss = (.*?);' ['Name: loss, Regex: Loss = (.*?);' ...]]>\n",
       "                        A list of dictionaries that defines the metric(s) used\n",
       "                        to evaluate the training jobs. Each dictionary\n",
       "                        contains two keys: ‘Name’ for the name of the metric,\n",
       "                        and ‘Regex’ for the regular expression used to extract\n",
       "                        the metric from the logs. This should be defined only\n",
       "                        for jobs that don’t use an Amazon algorithm.\n",
       "\n",
       "list:\n",
       "  --name_contains NAME_CONTAINS\n",
       "  --max_result MAX_RESULT\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/sage_maker_kernel/kernelmagics.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pytorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name workshop-sagemaker to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit:\n",
      " {\n",
      "    \"channel_testing\": \"s3://sagemaker-eu-west-1-245582572290/mnist\",\n",
      "    \"channel_training\": \"s3://sagemaker-eu-west-1-245582572290/mnist\",\n",
      "    \"enable_sagemaker_metrics\": false,\n",
      "    \"entry_point\": \"/tmp/tmp-c1179bc6-e175-43c2-9a6c-fb8229a02302.py\",\n",
      "    \"estimator_name\": \"pytest\",\n",
      "    \"framework_version\": \"1.5.0\",\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"batch-size\": \"128\",\n",
      "        \"epochs\": \"20\",\n",
      "        \"learning-rate\": \"1e-3\",\n",
      "        \"log-interval\": \"100\"\n",
      "    },\n",
      "    \"instance_count\": 1,\n",
      "    \"instance_type\": \"ml.c4.xlarge\",\n",
      "    \"max_result\": 10,\n",
      "    \"name_contains\": \"pytorch\",\n",
      "    \"output_path\": \"s3://sagemaker-eu-west-1-245582572290/pytorch/mnist\",\n",
      "    \"py_version\": \"py3\",\n",
      "    \"role\": \"arn:aws:iam::245582572290:role/workshop-sagemaker\",\n",
      "    \"use_spot_instances\": false\n",
      "}\n",
      "{\n",
      "    \"___PyTorch_latest_training_job_name\": \"pytorch-training-2020-12-10-13-18-07-907\",\n",
      "    \"estimator_variable\": \"pytest\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%pytorch submit --output_path s3://sagemaker-eu-west-1-245582572290/pytorch/mnist --channel_training s3://sagemaker-eu-west-1-245582572290/mnist --channel_testing s3://sagemaker-eu-west-1-245582572290/mnist --hyperparameters batch-size:128,epochs:20,learning-rate:1e-3,log-interval:100,backend:gloo  \n",
    "# --instance_type ml.g4dn.xlarge --instance_count 2 \n",
    "# --instance_type ml.c4.xlarge --instance_count 2 \n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Decode binary data from SM_CHANNEL_TRAINING\n",
    "# Decode and preprocess data\n",
    "# Create map dataset\n",
    "\n",
    "\n",
    "def normalize(x, axis):\n",
    "    eps = np.finfo(float).eps\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    # avoid division by zero\n",
    "    std = np.std(x, axis=axis, keepdims=True) + eps\n",
    "    return (x - mean) / std\n",
    "\n",
    "def convert_to_tensor(data_dir, images_file, labels_file):\n",
    "    \"\"\"Byte string to torch tensor \n",
    "    \"\"\"\n",
    "    with gzip.open(os.path.join(data_dir, images_file), 'rb') as f:\n",
    "        images = np.frombuffer(f.read(), \n",
    "                np.uint8, offset=16).reshape(-1, 28, 28).astype(np.float32)\n",
    "\n",
    "    with gzip.open(os.path.join(data_dir, labels_file), 'rb') as f:\n",
    "        labels = np.frombuffer(f.read(), np.uint8, offset=8).astype(\n",
    "                np.int64)\n",
    "        \n",
    "    # normalize the images\n",
    "    images = normalize(images, axis=(1,2))\n",
    "\n",
    "    # add channel dimension (depth-major)\n",
    "    images = np.expand_dims(images, axis=1)\n",
    "\n",
    "    # to torch tensor\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    return images, labels \n",
    "\n",
    "        \n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "\n",
    "        if train:\n",
    "            images_file=\"train-images-idx3-ubyte.gz\"\n",
    "            labels_file=\"train-labels-idx1-ubyte.gz\"\n",
    "        else:\n",
    "            images_file=\"t10k-images-idx3-ubyte.gz\"\n",
    "            labels_file=\"t10k-labels-idx1-ubyte.gz\"\n",
    "        \n",
    "        self.images, self.labels = convert_to_tensor(\n",
    "                data_dir, images_file, labels_file)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Initialize the distributed environment.\n",
    "    if(len(args.hosts)>1):\n",
    "        world_size = len(args.hosts)\n",
    "        os.environ['WORLD_SIZE'] = str(world_size)\n",
    "        host_rank = args.hosts.index(args.current_host)\n",
    "        dist.init_process_group(backend=args.backend, rank=host_rank)\n",
    "    \n",
    "    # GPU,  CPU\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    device = torch.device(\"cuda\" if use_cuda > 0 else \"cpu\")\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader = DataLoader(MNIST(args.train, train=True), \n",
    "            batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(MNIST(args.test, train=False),\n",
    "            batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "    net = Net().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), \n",
    "            betas=(args.beta_1, args.beta_2),\n",
    "            weight_decay=args.weight_decay)\n",
    "\n",
    "    logger.info(\"Start training ...\")    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        net.train()\n",
    "        for batch_idx, (imgs, labels) in enumerate(train_loader, 1):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = net(imgs)\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(imgs), len(train_loader.sampler),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "        # test the model\n",
    "        test(net, test_loader, device)\n",
    "\n",
    "    # save model checkpoint\n",
    "    save_model(net, args.model_dir)\n",
    "    return\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = model(imgs)\n",
    "            test_loss+=F.cross_entropy(output, labels, reduction='sum').item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    logger.info('Test set: Average loss: {:.4f}, Accuracy: {}/{}, {})\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100.0 * correct / len(test_loader.dataset)\n",
    "        ))\n",
    "    return\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    logger.info('Saving the model')\n",
    "    path = os.path.join(model_dir, 'model.pth')\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "    return\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 1)')\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--beta_1', type=float, default=0.9, metavar='BETA1',\n",
    "                        help='beta1 (default: 0.9)')\n",
    "    parser.add_argument('--beta_2', type=float, default=0.999, metavar='BETA2',\n",
    "                        help='beta2 (default: 0.999)')\n",
    "    parser.add_argument('--weight-decay', type=float, default=1e-4, metavar='WD',\n",
    "                        help='L2 weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TESTING'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "        \n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n"
     ]
    }
   ],
   "source": [
    "%pytorch delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"EnableSageMakerMetricsTimeSeries\": false,\n",
      "        \"TrainingImage\": \"763104351884.dkr.ecr.eu-west-1.amazonaws.com/pytorch-training:1.5.0-cpu-py3\",\n",
      "        \"TrainingInputMode\": \"File\"\n",
      "    },\n",
      "    \"CreationTime\": \"2020-12-10 13:18:08.259000+00:00\",\n",
      "    \"DebugHookConfig\": {\n",
      "        \"CollectionConfigurations\": [],\n",
      "        \"S3OutputPath\": \"s3://sagemaker-eu-west-1-245582572290/pytorch/mnist\"\n",
      "    },\n",
      "    \"EnableInterContainerTrafficEncryption\": false,\n",
      "    \"EnableManagedSpotTraining\": false,\n",
      "    \"EnableNetworkIsolation\": false,\n",
      "    \"HyperParameters\": {\n",
      "        \"backend\": \"\\\"gloo\\\"\",\n",
      "        \"batch-size\": \"\\\"128\\\"\",\n",
      "        \"epochs\": \"\\\"20\\\"\",\n",
      "        \"learning-rate\": \"\\\"1e-3\\\"\",\n",
      "        \"log-interval\": \"\\\"100\\\"\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"pytorch-training-2020-12-10-13-18-07-907\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"tmp-c1179bc6-e175-43c2-9a6c-fb8229a02302.py\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"eu-west-1\\\"\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-10-13-18-07-907/source/sourcedir.tar.gz\\\"\"\n",
      "    },\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"ChannelName\": \"training\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-eu-west-1-245582572290/mnist\"\n",
      "                }\n",
      "            },\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        {\n",
      "            \"ChannelName\": \"testing\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-eu-west-1-245582572290/mnist\"\n",
      "                }\n",
      "            },\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    ],\n",
      "    \"LastModifiedTime\": \"2020-12-10 13:18:19.610000+00:00\",\n",
      "    \"OutputDataConfig\": {\n",
      "        \"KmsKeyId\": \"\",\n",
      "        \"S3OutputPath\": \"s3://sagemaker-eu-west-1-245582572290/pytorch/mnist\"\n",
      "    },\n",
      "    \"ProfilerConfig\": {\n",
      "        \"ProfilingIntervalInMilliseconds\": 500,\n",
      "        \"S3OutputPath\": \"s3://sagemaker-eu-west-1-245582572290/pytorch/mnist\"\n",
      "    },\n",
      "    \"ProfilerRuleConfigurations\": [\n",
      "        {\n",
      "            \"RuleConfigurationName\": \"ProfilerReport-1607606287\",\n",
      "            \"RuleEvaluatorImage\": \"929884845733.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-debugger-rules:latest\",\n",
      "            \"RuleParameters\": {\n",
      "                \"rule_to_invoke\": \"ProfilerReport\"\n",
      "            },\n",
      "            \"VolumeSizeInGB\": 0\n",
      "        }\n",
      "    ],\n",
      "    \"ProfilerRuleEvaluationStatuses\": [\n",
      "        {\n",
      "            \"LastModifiedTime\": \"2020-12-10 13:18:09.608000+00:00\",\n",
      "            \"RuleConfigurationName\": \"ProfilerReport-1607606287\",\n",
      "            \"RuleEvaluationStatus\": \"InProgress\"\n",
      "        }\n",
      "    ],\n",
      "    \"ProfilingStatus\": \"Enabled\",\n",
      "    \"ResourceConfig\": {\n",
      "        \"InstanceCount\": 1,\n",
      "        \"InstanceType\": \"ml.c4.xlarge\",\n",
      "        \"VolumeSizeInGB\": 30\n",
      "    },\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"3138\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 10 Dec 2020 13:18:29 GMT\",\n",
      "            \"x-amzn-requestid\": \"915f552b-0b05-42d0-84f1-6526c6ad14b5\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"915f552b-0b05-42d0-84f1-6526c6ad14b5\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::245582572290:role/workshop-sagemaker\",\n",
      "    \"SecondaryStatus\": \"Starting\",\n",
      "    \"SecondaryStatusTransitions\": [\n",
      "        {\n",
      "            \"StartTime\": \"2020-12-10 13:18:08.259000+00:00\",\n",
      "            \"Status\": \"Starting\",\n",
      "            \"StatusMessage\": \"Launching requested ML instances\"\n",
      "        }\n",
      "    ],\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"TrainingJobArn\": \"arn:aws:sagemaker:eu-west-1:245582572290:training-job/pytorch-training-2020-12-10-13-18-07-907\",\n",
      "    \"TrainingJobName\": \"pytorch-training-2020-12-10-13-18-07-907\",\n",
      "    \"TrainingJobStatus\": \"Stopping\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%pytorch status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 13:25:48 Starting - Preparing the instances for training\n",
      "2020-12-10 13:25:48 Downloading - Downloading input data\n",
      "2020-12-10 13:25:48 Stopping - Stopping the training job\n",
      "2020-12-10 13:25:48 Stopped - Training job stopped.null\n"
     ]
    }
   ],
   "source": [
    "%pytorch logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"NextToken\": \"cIws2QhTXUIa8bi8X9aU7gCAR0Xdc3x9L/Ofg4vsVMTtcNqRqLcpBqE42+cDc29TFQi5WMnklKsi9+KMC+HgKs85fkvyr3CImKWVifwr6jTBNdmdg5BcotEju7OZ4HHHQu7Rq7mHpCbJf2DMzcOTEi4jSvTdQtZk9GspPq7plTKmeeDU/inEg6tFZbVYGimmLJeSuRHDWwwN+WEr0bHkRO4jcf+L1JP/mVFXEUCFq2Wmdy9XppAseQFJ1qHBT1JiYTFwvKyvgP3nuBrntpDqwmL3dSuIpFvMLwINeLWXn0wrJ+YQH8Ob5tDJz4tt1mto7PuYAlA8YlRxWA59NwkPws6Y57c2fioCp8UUOen7U6ZSA1SaVhnQGeU2Zj1Ea/cb9iEYqEgK9XN9Dtsz375Y5Ca06NQtGZCAxI1jciTDCPNFsVK6pPQk8S4VbY4Im0YgIZb/qdO6JS5kZNWxKxK2sHdy9JJTitBV7vPwc4xRcrc/CMra/Fs/K4kzaOq15ExWyCVpObICwJEwscMsBwxpIFf3kXpwdOJ2aEOygXVtXUqH1/nWuENqrCI2mrSF4HWfAqPEgwuNrAx9iixc9ldlx0QMdzNshXI7Pj8CjBejthxf\",\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"1591\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Thu, 10 Dec 2020 13:32:04 GMT\",\n",
      "            \"x-amzn-requestid\": \"d9ba2ace-b3b2-45ac-b609-3e8b575f4f16\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"d9ba2ace-b3b2-45ac-b609-3e8b575f4f16\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"TrainingJobSummaries\": [\n",
      "        {\n",
      "            \"CreationTime\": \"2020-12-10 02:52:52.482000+00:00\",\n",
      "            \"LastModifiedTime\": \"2020-12-10 03:00:15.263000+00:00\",\n",
      "            \"TrainingEndTime\": \"2020-12-10 02:55:27.924000+00:00\",\n",
      "            \"TrainingJobArn\": \"arn:aws:sagemaker:eu-west-1:245582572290:training-job/pytorch-training-2020-12-10-02-52-52-166\",\n",
      "            \"TrainingJobName\": \"pytorch-training-2020-12-10-02-52-52-166\",\n",
      "            \"TrainingJobStatus\": \"Stopped\"\n",
      "        },\n",
      "        {\n",
      "            \"CreationTime\": \"2020-12-10 02:31:32.012000+00:00\",\n",
      "            \"LastModifiedTime\": \"2020-12-10 02:39:11.093000+00:00\",\n",
      "            \"TrainingEndTime\": \"2020-12-10 02:34:25.333000+00:00\",\n",
      "            \"TrainingJobArn\": \"arn:aws:sagemaker:eu-west-1:245582572290:training-job/pytorch-training-2020-12-10-02-31-31-681\",\n",
      "            \"TrainingJobName\": \"pytorch-training-2020-12-10-02-31-31-681\",\n",
      "            \"TrainingJobStatus\": \"Stopped\"\n",
      "        },\n",
      "        {\n",
      "            \"CreationTime\": \"2020-12-10 02:04:24.741000+00:00\",\n",
      "            \"LastModifiedTime\": \"2020-12-10 02:11:50.472000+00:00\",\n",
      "            \"TrainingEndTime\": \"2020-12-10 02:07:41.669000+00:00\",\n",
      "            \"TrainingJobArn\": \"arn:aws:sagemaker:eu-west-1:245582572290:training-job/pytorch-training-2020-12-10-02-04-24-474\",\n",
      "            \"TrainingJobName\": \"pytorch-training-2020-12-10-02-04-24-474\",\n",
      "            \"TrainingJobStatus\": \"Stopped\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%pytorch list --name_contains pytorch-training-2020-12-10-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 16:33:06 Starting - Preparing the instances for training\n",
      "2020-12-04 16:33:06 Downloading - Downloading input data\n",
      "2020-12-04 16:33:06 Training - Training image download completed. Training in progress.\n",
      "2020-12-04 16:33:06 Uploading - Uploading generated training model\n",
      "2020-12-04 16:33:06 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-04 16:28:58,617 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-04 16:28:58,620 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-04 16:28:58,632 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:07,434 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:07,436 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:07,448 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:10,464 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:10,792 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:10,792 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:10,792 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:10,793 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:11,279 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:11,608 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:11,609 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:11,609 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:11,609 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpt7vfer8r/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpkm9267cs/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=8478 sha256=e82dad49ee4f0f9fd76e387e08785fc097457e513dd3ae89c890dbe3e1152d01\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ubr74k1r/wheels/81/2e/ce/8426e6787c95a3042d084f7fa73962bb9da08c795b5123a620\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=8478 sha256=e82dad49ee4f0f9fd76e387e08785fc097457e513dd3ae89c890dbe3e1152d01\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-junf6aje/wheels/f7/d3/55/d4c5b6d07f827e1e8bbe91bba2b4a69afff213a29df8957e69\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:13,138 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:13,152 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:13,166 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-12-04 16:29:13,178 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": \"128\",\n",
      "        \"log-interval\": \"100\",\n",
      "        \"learning-rate\": \"1e-3\",\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": \"20\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-12-04-16-25-36-311\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-16-25-36-311/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-16-25-36-311/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-12-04-16-25-36-311\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-16-25-36-311/source/sourcedir.tar.gz\",\"module_name\":\"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"128\",\"--epochs\",\"20\",\"--learning-rate\",\"1e-3\",\"--log-interval\",\"100\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[35mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING-RATE=1e-3\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py --backend gloo --batch-size 128 --epochs 20 --learning-rate 1e-3 --log-interval 100\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:13,958 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:13,973 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:13,986 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-04 16:29:13,998 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": \"128\",\n",
      "        \"log-interval\": \"100\",\n",
      "        \"learning-rate\": \"1e-3\",\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": \"20\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-12-04-16-25-36-311\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-16-25-36-311/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-16-25-36-311/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-12-04-16-25-36-311\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-16-25-36-311/source/sourcedir.tar.gz\",\"module_name\":\"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"128\",\"--epochs\",\"20\",\"--learning-rate\",\"1e-3\",\"--log-interval\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=1e-3\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python tmp-150a75dc-4bd0-43a5-aade-f01f9b59cca8.py --backend gloo --batch-size 128 --epochs 20 --learning-rate 1e-3 --log-interval 100\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mStart training ...\u001b[0m\n",
      "\u001b[35mStart training ...\u001b[0m\n",
      "\u001b[34m[2020-12-04 16:29:17.243 algo-1:44 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-04 16:29:17.244 algo-1:44 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-04 16:29:17.244 algo-1:44 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-04 16:29:17.244 algo-1:44 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m[2020-12-04 16:29:17.217 algo-2:45 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2020-12-04 16:29:17.218 algo-2:45 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2020-12-04 16:29:17.218 algo-2:45 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2020-12-04 16:29:17.218 algo-2:45 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.571117\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.571117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.435707\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.435707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.278377\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.278377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.247071\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.247071\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1151, Accuracy: 9642/10000, 96.42)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1151, Accuracy: 9642/10000, 96.42)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.245921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.245921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.197355\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.197355\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.285320\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.285320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.146226\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.146226\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0750, Accuracy: 9772/10000, 97.72)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0750, Accuracy: 9772/10000, 97.72)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.221168\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.221168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.365144\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.365144\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.299798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.299798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.162260\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.162260\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0643, Accuracy: 9800/10000, 98.0)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0643, Accuracy: 9800/10000, 98.0)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.230822\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.230822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.078995\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.078995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.151933\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.151933\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.110981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.110981\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0543, Accuracy: 9839/10000, 98.39)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0543, Accuracy: 9839/10000, 98.39)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.245588\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.245588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.094907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.094907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.110125\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.110125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.095660\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.095660\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0480, Accuracy: 9858/10000, 98.58)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0480, Accuracy: 9858/10000, 98.58)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)] Loss: 0.164628\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12800/60000 (21%)] Loss: 0.164628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)] Loss: 0.123027\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [25600/60000 (43%)] Loss: 0.123027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)] Loss: 0.117041\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [38400/60000 (64%)] Loss: 0.117041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)] Loss: 0.189925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [51200/60000 (85%)] Loss: 0.189925\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0455, Accuracy: 9848/10000, 98.48)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0455, Accuracy: 9848/10000, 98.48)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)] Loss: 0.247882\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [12800/60000 (21%)] Loss: 0.247882\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [25600/60000 (43%)] Loss: 0.127230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)] Loss: 0.127230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)] Loss: 0.140743\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [38400/60000 (64%)] Loss: 0.140743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)] Loss: 0.145459\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [51200/60000 (85%)] Loss: 0.145459\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0439, Accuracy: 9864/10000, 98.64)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0439, Accuracy: 9864/10000, 98.64)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)] Loss: 0.110049\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [12800/60000 (21%)] Loss: 0.110049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)] Loss: 0.083448\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [25600/60000 (43%)] Loss: 0.083448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)] Loss: 0.191708\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [38400/60000 (64%)] Loss: 0.191708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)] Loss: 0.130014\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [51200/60000 (85%)] Loss: 0.130014\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0403, Accuracy: 9878/10000, 98.78)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0403, Accuracy: 9878/10000, 98.78)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)] Loss: 0.120447\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [12800/60000 (21%)] Loss: 0.120447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)] Loss: 0.104431\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [25600/60000 (43%)] Loss: 0.104431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)] Loss: 0.203298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [38400/60000 (64%)] Loss: 0.203298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)] Loss: 0.111792\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [51200/60000 (85%)] Loss: 0.111792\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0388, Accuracy: 9878/10000, 98.78)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0388, Accuracy: 9878/10000, 98.78)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)] Loss: 0.101812\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [12800/60000 (21%)] Loss: 0.101812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)] Loss: 0.253433\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [25600/60000 (43%)] Loss: 0.253433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)] Loss: 0.204823\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [38400/60000 (64%)] Loss: 0.204823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)] Loss: 0.114251\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [51200/60000 (85%)] Loss: 0.114251\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0390, Accuracy: 9883/10000, 98.83)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0390, Accuracy: 9883/10000, 98.83)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [12800/60000 (21%)] Loss: 0.083494\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [12800/60000 (21%)] Loss: 0.083494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [25600/60000 (43%)] Loss: 0.057258\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [25600/60000 (43%)] Loss: 0.057258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [38400/60000 (64%)] Loss: 0.107252\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [38400/60000 (64%)] Loss: 0.107252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [51200/60000 (85%)] Loss: 0.121119\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [51200/60000 (85%)] Loss: 0.121119\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0358, Accuracy: 9890/10000, 98.9)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0358, Accuracy: 9890/10000, 98.9)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [12800/60000 (21%)] Loss: 0.066673\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [12800/60000 (21%)] Loss: 0.066673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [25600/60000 (43%)] Loss: 0.091743\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [25600/60000 (43%)] Loss: 0.091743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [38400/60000 (64%)] Loss: 0.083124\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [38400/60000 (64%)] Loss: 0.083124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [51200/60000 (85%)] Loss: 0.121673\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [51200/60000 (85%)] Loss: 0.121673\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0358, Accuracy: 9882/10000, 98.82)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0358, Accuracy: 9882/10000, 98.82)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [12800/60000 (21%)] Loss: 0.158788\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [12800/60000 (21%)] Loss: 0.158788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [25600/60000 (43%)] Loss: 0.120389\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [25600/60000 (43%)] Loss: 0.120389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [38400/60000 (64%)] Loss: 0.086373\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [38400/60000 (64%)] Loss: 0.086373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [51200/60000 (85%)] Loss: 0.146852\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [51200/60000 (85%)] Loss: 0.146852\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0352, Accuracy: 9881/10000, 98.81)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0352, Accuracy: 9881/10000, 98.81)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [12800/60000 (21%)] Loss: 0.083338\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [12800/60000 (21%)] Loss: 0.083338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [25600/60000 (43%)] Loss: 0.137741\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [25600/60000 (43%)] Loss: 0.137741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [38400/60000 (64%)] Loss: 0.119579\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [38400/60000 (64%)] Loss: 0.119579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [51200/60000 (85%)] Loss: 0.156062\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [51200/60000 (85%)] Loss: 0.156062\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0333, Accuracy: 9901/10000, 99.01)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0333, Accuracy: 9901/10000, 99.01)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [12800/60000 (21%)] Loss: 0.178140\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [12800/60000 (21%)] Loss: 0.178140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [25600/60000 (43%)] Loss: 0.057751\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [25600/60000 (43%)] Loss: 0.057751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [38400/60000 (64%)] Loss: 0.271318\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [38400/60000 (64%)] Loss: 0.271318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [51200/60000 (85%)] Loss: 0.113298\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [51200/60000 (85%)] Loss: 0.113298\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0318, Accuracy: 9897/10000, 98.97)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0318, Accuracy: 9897/10000, 98.97)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [12800/60000 (21%)] Loss: 0.137096\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [12800/60000 (21%)] Loss: 0.137096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [25600/60000 (43%)] Loss: 0.090337\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [25600/60000 (43%)] Loss: 0.090337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [38400/60000 (64%)] Loss: 0.109978\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [38400/60000 (64%)] Loss: 0.109978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [51200/60000 (85%)] Loss: 0.088726\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [51200/60000 (85%)] Loss: 0.088726\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0332, Accuracy: 9902/10000, 99.02)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0332, Accuracy: 9902/10000, 99.02)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [12800/60000 (21%)] Loss: 0.154219\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [12800/60000 (21%)] Loss: 0.154219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [25600/60000 (43%)] Loss: 0.120173\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [25600/60000 (43%)] Loss: 0.120173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [38400/60000 (64%)] Loss: 0.110400\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [38400/60000 (64%)] Loss: 0.110400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [51200/60000 (85%)] Loss: 0.142481\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [51200/60000 (85%)] Loss: 0.142481\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0350, Accuracy: 9888/10000, 98.88)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0350, Accuracy: 9888/10000, 98.88)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [12800/60000 (21%)] Loss: 0.080115\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [12800/60000 (21%)] Loss: 0.080115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [25600/60000 (43%)] Loss: 0.133742\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [25600/60000 (43%)] Loss: 0.133742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [38400/60000 (64%)] Loss: 0.099172\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [38400/60000 (64%)] Loss: 0.099172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [51200/60000 (85%)] Loss: 0.078593\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [51200/60000 (85%)] Loss: 0.078593\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0312, Accuracy: 9896/10000, 98.96)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0312, Accuracy: 9896/10000, 98.96)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [12800/60000 (21%)] Loss: 0.068928\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [12800/60000 (21%)] Loss: 0.068928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [25600/60000 (43%)] Loss: 0.068705\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [25600/60000 (43%)] Loss: 0.068705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [38400/60000 (64%)] Loss: 0.155928\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [38400/60000 (64%)] Loss: 0.155928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [51200/60000 (85%)] Loss: 0.067561\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [51200/60000 (85%)] Loss: 0.067561\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0332, Accuracy: 9888/10000, 98.88)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0332, Accuracy: 9888/10000, 98.88)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [12800/60000 (21%)] Loss: 0.170418\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [12800/60000 (21%)] Loss: 0.170418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [25600/60000 (43%)] Loss: 0.060078\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [25600/60000 (43%)] Loss: 0.060078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [38400/60000 (64%)] Loss: 0.105496\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [38400/60000 (64%)] Loss: 0.105496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [51200/60000 (85%)] Loss: 0.108972\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [51200/60000 (85%)] Loss: 0.108972\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0317, Accuracy: 9896/10000, 98.96)\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model\u001b[0m\n",
      "\u001b[34m[2020-12-04 16:32:52.352 algo-1:44 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-12-04 16:32:52,516 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0317, Accuracy: 9896/10000, 98.96)\n",
      "\u001b[0m\n",
      "\u001b[35mSaving the model\u001b[0m\n",
      "\u001b[35m[2020-12-04 16:32:52.765 algo-2:45 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[35m2020-12-04 16:32:52,932 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 590\n",
      "Billable seconds: 590\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Session\n",
    "\n",
    "Session().logs_for_job('pytorch-training-2020-12-04-16-25-36-311', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 18:08:44 Starting - Preparing the instances for training\n",
      "2020-12-04 18:08:44 Downloading - Downloading input data\n",
      "2020-12-04 18:08:44 Training - Training image download completed. Training in progress.\n",
      "2020-12-04 18:08:44 Uploading - Uploading generated training model\n",
      "2020-12-04 18:08:44 Completed - Training job completed\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:40,352 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:40,373 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:38,834 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:38,855 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:43,386 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:43,686 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:43,686 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:43,687 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:43,687 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmp2h4_5gmc/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=8716 sha256=925fd49c26670ce087dc76d79db9ef363b611b69b01c7dfeec9d1edd6366dd87\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uum2p_x9/wheels/7e/23/b8/cd2a431cf52e38212420f9a11369ed9c82922613bade2ace13\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-12-04 18:07:45,618 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": \"128\",\n",
      "        \"log-interval\": \"100\",\n",
      "        \"learning-rate\": \"1e-3\",\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": \"20\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-12-04-18-03-55-165\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-18-03-55-165/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-18-03-55-165/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-12-04-18-03-55-165\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-18-03-55-165/source/sourcedir.tar.gz\",\"module_name\":\"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"128\",\"--epochs\",\"20\",\"--learning-rate\",\"1e-3\",\"--log-interval\",\"100\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[35mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING-RATE=1e-3\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py --backend gloo --batch-size 128 --epochs 20 --learning-rate 1e-3 --log-interval 100\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:45,075 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:45,372 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:45,373 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:45,373 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:45,373 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpr4czg574/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=8716 sha256=2e289fd4e2ab74edc8dfd9d75ddeda67c48891b7f0c62691a6a470c45d2b90a6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fv99qdy9/wheels/b9/c2/85/fff20a398123255ba484f8118f190efd556783b5836f71e174\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-12-04 18:07:47,448 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": \"128\",\n",
      "        \"log-interval\": \"100\",\n",
      "        \"learning-rate\": \"1e-3\",\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": \"20\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-12-04-18-03-55-165\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-18-03-55-165/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-18-03-55-165/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":\"128\",\"epochs\":\"20\",\"learning-rate\":\"1e-3\",\"log-interval\":\"100\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-12-04-18-03-55-165\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-245582572290/pytorch-training-2020-12-04-18-03-55-165/source/sourcedir.tar.gz\",\"module_name\":\"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"128\",\"--epochs\",\"20\",\"--learning-rate\",\"1e-3\",\"--log-interval\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=1e-3\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python tmp-3f5d52d4-aae6-475b-a225-c0def6a8daaa.py --backend gloo --batch-size 128 --epochs 20 --learning-rate 1e-3 --log-interval 100\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mStart training ...\u001b[0m\n",
      "\u001b[35m[2020-12-04 18:07:53.377 algo-2:42 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2020-12-04 18:07:53.378 algo-2:42 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2020-12-04 18:07:53.378 algo-2:42 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2020-12-04 18:07:53.378 algo-2:42 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mStart training ...\u001b[0m\n",
      "\u001b[34m[2020-12-04 18:07:53.932 algo-1:42 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-04 18:07:53.932 algo-1:42 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-04 18:07:53.932 algo-1:42 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-04 18:07:53.933 algo-1:42 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.585010\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.337120\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.351411\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.316778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.585010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.337120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.351411\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1146, Accuracy: 9656/10000, 96.56)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.339385\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.300610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.316778\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1146, Accuracy: 9656/10000, 96.56)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.166998\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.156052\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0786, Accuracy: 9763/10000, 97.63)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.339385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.300610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.166998\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.169352\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.199834\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.130711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.156052\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0786, Accuracy: 9763/10000, 97.63)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.169352\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.143861\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0685, Accuracy: 9779/10000, 97.79)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.199834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.130711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.143861\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.210724\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.292460\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.223437\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0685, Accuracy: 9779/10000, 97.79)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.210724\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.260931\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0595, Accuracy: 9824/10000, 98.24)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.161603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.292460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.223437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.260931\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.148179\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.296443\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.296254\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0595, Accuracy: 9824/10000, 98.24)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.161603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.148179\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0541, Accuracy: 9824/10000, 98.24)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12800/60000 (21%)] Loss: 0.121137\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [25600/60000 (43%)] Loss: 0.201755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.296443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.296254\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0541, Accuracy: 9824/10000, 98.24)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [38400/60000 (64%)] Loss: 0.172329\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [51200/60000 (85%)] Loss: 0.097607\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0490, Accuracy: 9852/10000, 98.52)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)] Loss: 0.121137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)] Loss: 0.201755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)] Loss: 0.172329\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [12800/60000 (21%)] Loss: 0.091047\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [25600/60000 (43%)] Loss: 0.181441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)] Loss: 0.097607\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0490, Accuracy: 9852/10000, 98.52)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [38400/60000 (64%)] Loss: 0.141001\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [51200/60000 (85%)] Loss: 0.058877\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0498, Accuracy: 9847/10000, 98.47)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)] Loss: 0.091047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)] Loss: 0.181441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)] Loss: 0.141001\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [12800/60000 (21%)] Loss: 0.133268\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [25600/60000 (43%)] Loss: 0.178669\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [38400/60000 (64%)] Loss: 0.205779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)] Loss: 0.058877\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0498, Accuracy: 9847/10000, 98.47)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)] Loss: 0.133268\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [51200/60000 (85%)] Loss: 0.214393\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0450, Accuracy: 9864/10000, 98.64)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [12800/60000 (21%)] Loss: 0.118170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)] Loss: 0.178669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)] Loss: 0.205779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)] Loss: 0.214393\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [25600/60000 (43%)] Loss: 0.145741\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [38400/60000 (64%)] Loss: 0.193997\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [51200/60000 (85%)] Loss: 0.180279\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0450, Accuracy: 9864/10000, 98.64)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)] Loss: 0.118170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)] Loss: 0.145741\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0434, Accuracy: 9878/10000, 98.78)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [12800/60000 (21%)] Loss: 0.093407\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [25600/60000 (43%)] Loss: 0.176685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)] Loss: 0.193997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)] Loss: 0.180279\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0434, Accuracy: 9878/10000, 98.78)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [38400/60000 (64%)] Loss: 0.150899\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [51200/60000 (85%)] Loss: 0.161937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)] Loss: 0.093407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)] Loss: 0.176685\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0397, Accuracy: 9879/10000, 98.79)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [12800/60000 (21%)] Loss: 0.067579\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [25600/60000 (43%)] Loss: 0.140798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)] Loss: 0.150899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)] Loss: 0.161937\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0397, Accuracy: 9879/10000, 98.79)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [38400/60000 (64%)] Loss: 0.094264\u001b[0m\n",
      "\u001b[35mTrain Epoch: 11 [51200/60000 (85%)] Loss: 0.126620\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0449, Accuracy: 9872/10000, 98.72)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [12800/60000 (21%)] Loss: 0.067579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [25600/60000 (43%)] Loss: 0.140798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [38400/60000 (64%)] Loss: 0.094264\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [12800/60000 (21%)] Loss: 0.138408\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [25600/60000 (43%)] Loss: 0.070859\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [38400/60000 (64%)] Loss: 0.124214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [51200/60000 (85%)] Loss: 0.126620\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0449, Accuracy: 9872/10000, 98.72)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [12800/60000 (21%)] Loss: 0.138408\u001b[0m\n",
      "\u001b[35mTrain Epoch: 12 [51200/60000 (85%)] Loss: 0.123156\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0398, Accuracy: 9870/10000, 98.7)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [12800/60000 (21%)] Loss: 0.171537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [25600/60000 (43%)] Loss: 0.070859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [38400/60000 (64%)] Loss: 0.124214\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [25600/60000 (43%)] Loss: 0.113798\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [38400/60000 (64%)] Loss: 0.065256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [51200/60000 (85%)] Loss: 0.123156\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0398, Accuracy: 9870/10000, 98.7)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [12800/60000 (21%)] Loss: 0.171537\u001b[0m\n",
      "\u001b[35mTrain Epoch: 13 [51200/60000 (85%)] Loss: 0.083888\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0364, Accuracy: 9893/10000, 98.93)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [12800/60000 (21%)] Loss: 0.122381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [25600/60000 (43%)] Loss: 0.113798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [38400/60000 (64%)] Loss: 0.065256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [51200/60000 (85%)] Loss: 0.083888\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [25600/60000 (43%)] Loss: 0.157306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [38400/60000 (64%)] Loss: 0.129081\u001b[0m\n",
      "\u001b[35mTrain Epoch: 14 [51200/60000 (85%)] Loss: 0.082023\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0364, Accuracy: 9893/10000, 98.93)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [12800/60000 (21%)] Loss: 0.122381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [25600/60000 (43%)] Loss: 0.157306\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0372, Accuracy: 9887/10000, 98.87)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [12800/60000 (21%)] Loss: 0.359336\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [25600/60000 (43%)] Loss: 0.094823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [38400/60000 (64%)] Loss: 0.129081\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [51200/60000 (85%)] Loss: 0.082023\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0372, Accuracy: 9887/10000, 98.87)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [38400/60000 (64%)] Loss: 0.198466\u001b[0m\n",
      "\u001b[35mTrain Epoch: 15 [51200/60000 (85%)] Loss: 0.072843\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0355, Accuracy: 9888/10000, 98.88)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [12800/60000 (21%)] Loss: 0.359336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [25600/60000 (43%)] Loss: 0.094823\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [12800/60000 (21%)] Loss: 0.077492\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [25600/60000 (43%)] Loss: 0.189444\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [38400/60000 (64%)] Loss: 0.107904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [38400/60000 (64%)] Loss: 0.198466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [51200/60000 (85%)] Loss: 0.072843\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0355, Accuracy: 9888/10000, 98.88)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 16 [51200/60000 (85%)] Loss: 0.111026\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0354, Accuracy: 9895/10000, 98.95)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [12800/60000 (21%)] Loss: 0.077492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [25600/60000 (43%)] Loss: 0.189444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [38400/60000 (64%)] Loss: 0.107904\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [12800/60000 (21%)] Loss: 0.173549\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [25600/60000 (43%)] Loss: 0.114993\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [38400/60000 (64%)] Loss: 0.050758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [51200/60000 (85%)] Loss: 0.111026\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0354, Accuracy: 9895/10000, 98.95)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [12800/60000 (21%)] Loss: 0.173549\u001b[0m\n",
      "\u001b[35mTrain Epoch: 17 [51200/60000 (85%)] Loss: 0.196594\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0342, Accuracy: 9891/10000, 98.91)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [12800/60000 (21%)] Loss: 0.050653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [25600/60000 (43%)] Loss: 0.114993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [38400/60000 (64%)] Loss: 0.050758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [51200/60000 (85%)] Loss: 0.196594\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [25600/60000 (43%)] Loss: 0.144669\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [38400/60000 (64%)] Loss: 0.059623\u001b[0m\n",
      "\u001b[35mTrain Epoch: 18 [51200/60000 (85%)] Loss: 0.146155\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0342, Accuracy: 9891/10000, 98.91)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [12800/60000 (21%)] Loss: 0.050653\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0351, Accuracy: 9897/10000, 98.97)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [12800/60000 (21%)] Loss: 0.143751\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [25600/60000 (43%)] Loss: 0.124642\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [25600/60000 (43%)] Loss: 0.144669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [38400/60000 (64%)] Loss: 0.059623\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [51200/60000 (85%)] Loss: 0.146155\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [38400/60000 (64%)] Loss: 0.052907\u001b[0m\n",
      "\u001b[35mTrain Epoch: 19 [51200/60000 (85%)] Loss: 0.147885\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0317, Accuracy: 9893/10000, 98.93)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0351, Accuracy: 9897/10000, 98.97)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [12800/60000 (21%)] Loss: 0.143751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [25600/60000 (43%)] Loss: 0.124642\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [12800/60000 (21%)] Loss: 0.139109\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [25600/60000 (43%)] Loss: 0.171674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [38400/60000 (64%)] Loss: 0.052907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [51200/60000 (85%)] Loss: 0.147885\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0317, Accuracy: 9893/10000, 98.93)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [38400/60000 (64%)] Loss: 0.149039\u001b[0m\n",
      "\u001b[35mTrain Epoch: 20 [51200/60000 (85%)] Loss: 0.080656\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0371, Accuracy: 9879/10000, 98.79)\n",
      "\u001b[0m\n",
      "\u001b[35mSaving the model\u001b[0m\n",
      "\u001b[35m[2020-12-04 18:08:31.588 algo-2:42 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [12800/60000 (21%)] Loss: 0.139109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [25600/60000 (43%)] Loss: 0.171674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [38400/60000 (64%)] Loss: 0.149039\u001b[0m\n",
      "\u001b[35m2020-12-04 18:08:31,997 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [51200/60000 (85%)] Loss: 0.080656\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0371, Accuracy: 9879/10000, 98.79)\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model\u001b[0m\n",
      "\u001b[34m[2020-12-04 18:08:32.481 algo-1:42 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-12-04 18:08:32,857 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 326\n",
      "Billable seconds: 326\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Session\n",
    "\n",
    "Session().logs_for_job('pytorch-training-2020-12-04-18-03-55-165', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sm (lblokhin/20)",
   "language": "python",
   "name": "sm__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:245582572290:image-version/lblokhin/20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "sm_kernel",
   "pygments_lexer": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
